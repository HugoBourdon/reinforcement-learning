{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4cdcb57",
   "metadata": {},
   "source": [
    "# RL6 - Notebook Actor-Critic Algorithms\n",
    "\n",
    "By Hedwin BONNAVAUD - hedwin.bonnavaud@isae.fr\n",
    "\n",
    " 1. [Description](#description)\n",
    " 2. [Prerequisites](#prerequisites)\n",
    " 2. [Actor-critic](#actor_critic)\n",
    "     1. [Deep Deterministic policy gradient](#ddpg)\n",
    "     2. [Soft-Actor Critic](#sac)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6a1e8b",
   "metadata": {},
   "source": [
    "## Description\n",
    "<div id=\"description\"></div>\n",
    "\n",
    "In the reinforcement basics notebook, you saw how an actor critic architecture can be used to learn an optimal policy in a given environment. This architecture is also mandatory when we want to do reinforcement learning in a continuous actions action space environment, and we are going to see why latter. Our goal in this notebook, is to implement two of the most famous actor-critic algorithms, DDPG and SAC, to solve continuous action space environments. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8d29ae",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93991838",
   "metadata": {},
   "source": [
    "### Environments\n",
    "\n",
    "First, let's introduce the environments we are going to use.\n",
    "\n",
    "#### [Pendulum-v0](https://gym.openai.com/envs/Pendulum-v0/)\n",
    "\n",
    "Here, the agent controll the force applied on the pendulum. The pendulum cannot move, but it can rotate aroud it's axis. His rotation speed depend on the force applied by the agent.\n",
    "\n",
    "The state of the cos and sin of the rotating part angle, and his angle.\n",
    "\n",
    "The action space is obviously continuous. In other work, this is a continuous and simpler (even if in general, continuous mean harder) version on the swing up environment we saw at the last class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a91ec20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "pendulum_environment = gym.make(\"Pendulum-v0\")\n",
    "\n",
    "print(\"action space:\", pendulum_environment.action_space)\n",
    "print(\"observation space:\", pendulum_environment.observation_space)\n",
    "\n",
    "observation = pendulum_environment.reset()\n",
    "pendulum_environment.render()\n",
    "for i in range(100):\n",
    "    _, _, done, _ = pendulum_environment.step(pendulum_environment.action_space.sample())\n",
    "    pendulum_environment.render()\n",
    "    if done:\n",
    "        break\n",
    "    time.sleep(0.01)\n",
    "\n",
    "\n",
    "pendulum_environment.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01af1ebe",
   "metadata": {},
   "source": [
    "#### [LunarLanderContinuous-v2](https://gym.openai.com/envs/LunarLanderContinuous-v2/)\n",
    "\n",
    "In this environment, our agent is a lunar lander space ship, that is trying to land on the moon on a specific place.\n",
    "\n",
    "The state space is here a bit complex, and contain informations about agent situation, like his angle, his rotation velocity, his horizontal and vertical velocity, his geographical position, ... The latest is a really important information because the landing pad is located at coordinate [0, 0].\n",
    "    \n",
    "The availables actions is the amount of gaz send in 3 directions (left, right, and bottom) and the more the agent send gaz in a direction, the more he will move in the opposite way. Actions made are visible on the random agent video that you will build by running the next cell.\n",
    "\n",
    "Note: left and right gaz are controlled by the same action, so the agent have an action space made of two continuous actions (click on the link in the environment name above for more information).\n",
    "\n",
    "If you want to play, you can find many other environments on [OpenAI gym](https://gym.openai.com/envs/#classic_control) website. Don't forget to check the documentation and look deeper into the source code to know if they have a continuous or discrete action space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df019f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "pendulum_environment = gym.make(\"LunarLanderContinuous-v2\")\n",
    "\n",
    "print(\"action space:\", pendulum_environment.action_space)\n",
    "print(\"observation space:\", pendulum_environment.observation_space)\n",
    "\n",
    "observation = pendulum_environment.reset()\n",
    "pendulum_environment.render()\n",
    "for i in range(100):\n",
    "    _, _, done, _ = pendulum_environment.step(pendulum_environment.action_space.sample())\n",
    "    pendulum_environment.render()\n",
    "    if done:\n",
    "        break\n",
    "    time.sleep(0.01)\n",
    "\n",
    "pendulum_environment.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb278307",
   "metadata": {},
   "source": [
    "### Agents\n",
    "\n",
    "To simplify the global code of this notebook, I propose you tho use a mother class for each of our agents. This will allow us to build a simulation function that take in argument an environment and an agent, and work with any agents and any environments. Then all you will have to do is to describe what the agent should do at each step of the learning.\n",
    "\n",
    "I give you here this class, with the replay buffer from RL5 notebook, and an implementation of DQN that follow ou main Agent structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c7b4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from statistics import mean\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, device=DEVICE):\n",
    "        self.capacity = capacity # capacity of the buffer\n",
    "        self.device = device\n",
    "        self.data = []\n",
    "        self.index = 0 # index of the next cell to be filled\n",
    "\n",
    "    def append(self, s, a, r, s_, d):\n",
    "        if len(self.data) < self.capacity:\n",
    "            self.data.append(None)\n",
    "        self.data[self.index] = (s, a, r, s_, d)\n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.data, batch_size)\n",
    "        return list(map(lambda x:torch.Tensor(x).to(self.device), list(zip(*batch))))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    An global agent class that describe the interactions between our agent and it's environment\n",
    "    \"\"\"\n",
    "    def __init__(self, state_space, action_space, device=DEVICE, name=\"base_agent\"):\n",
    "\n",
    "        self.name = name  # The name is used inside plot legend, outputs directory path, and outputs file names\n",
    "\n",
    "        self.state_space = state_space\n",
    "        self.state_shape = state_space.shape\n",
    "        self.state_size = state_space.shape[0]  # Assume state space is continuous\n",
    "\n",
    "        self.continuous = isinstance(action_space, gym.spaces.Box)\n",
    "        self.action_space = action_space\n",
    "        self.nb_actions = self.action_space.shape[0] if self.continuous else self.action_space.n\n",
    "        self.last_state = None  # Usefull to store interaction when we recieve (new_stare, reward, done) tuple\n",
    "        self.device = device\n",
    "        self.episode_id = 0\n",
    "        self.episode_time_step_id = 0\n",
    "        self.time_step_id = 0\n",
    "    \n",
    "    def on_simulation_start(self):\n",
    "        \"\"\"\n",
    "        Called when an episode is started. will be used by child class.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def on_episode_start(self, state):\n",
    "        self.last_state = state\n",
    "        self.episode_time_step_id = 0\n",
    "        self.episode_id = 0\n",
    "\n",
    "    def action(self, state):\n",
    "        res = self.action_space.sample()\n",
    "        return res\n",
    "\n",
    "    def on_action_stop(self, action, new_state, reward, done):\n",
    "        self.episode_time_step_id += 1\n",
    "        self.time_step_id += 1\n",
    "        self.last_state = new_state\n",
    "\n",
    "    def on_episode_stop(self):\n",
    "        self.episode_id += 1\n",
    "\n",
    "    def on_simulation_stop(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Now we can define our DQN using Agent class\n",
    "class DQNAgent(Agent):\n",
    "    \"\"\"\n",
    "    An agent that learn an approximated Q-Function using a neural network. \n",
    "    This Q-Function is used to find the best action to execute in a given state. \n",
    "    \"\"\"\n",
    "    def __init__(self, state_space, action_space, name=\"DQN\",\n",
    "                gamma=0.95, epsilon_min=0.01, epsilon_max=1., epsilon_decay_period=1000, epsilon_decay_delay=20,\n",
    "                buffer_size=1000000, learning_rate=0.001, update_target_freq=100, batch_size=20,\n",
    "                layer_1_size=50, layer_2_size=50, nb_gradient_steps=1):\n",
    "        \n",
    "        assert isinstance(action_space, gym.spaces.Discrete)  # Make sure our action space is discrete\n",
    "        super().__init__(state_space, action_space, name=name)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_max = epsilon_max\n",
    "        self.epsilon_decay_delay = epsilon_decay_delay\n",
    "        \n",
    "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.update_target_freq = update_target_freq\n",
    "        self.nb_gradient_steps = nb_gradient_steps\n",
    "        \n",
    "        self.epsilon_step = (epsilon_max - self.epsilon_min) / epsilon_decay_period\n",
    "        self.total_steps = 0\n",
    "        self.model = torch.nn.Sequential(nn.Linear(self.state_size, layer_1_size),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(layer_1_size, layer_2_size),\n",
    "                          nn.ReLU(), \n",
    "                          nn.Linear(layer_2_size, self.nb_actions)).to(self.device)\n",
    "        \n",
    "        self.criterion = torch.nn.SmoothL1Loss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.target_model = copy.deepcopy(self.model).to(self.device)\n",
    "        self.update_target_freq = update_target_freq\n",
    "    \n",
    "    def on_simulation_start(self):\n",
    "        self.epsilon = self.epsilon_max\n",
    "    \n",
    "    def action(self, state):\n",
    "        if self.time_step_id > self.epsilon_decay_delay:                \n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon - self.epsilon_step)\n",
    "        \n",
    "        if np.random.rand() < self.epsilon:  # Epsilon greedy\n",
    "            action = np.random.randint(self.nb_actions)\n",
    "        else:\n",
    "            # greedy_action(self.model, state) function in RL5 notebook\n",
    "            with torch.no_grad():\n",
    "                Q = self.model(torch.Tensor(state).unsqueeze(0).to(self.device))\n",
    "                action = torch.argmax(Q).item()\n",
    "        return action\n",
    "\n",
    "    def on_action_stop(self, action, new_state, reward, done):\n",
    "        self.replay_buffer.append(self.last_state, action, reward, new_state, done)\n",
    "        self.learn()\n",
    "        super().on_action_stop(action, new_state, reward, done)  # Repale self.last_state by the new_state\n",
    "    def learn(self):\n",
    "        for _ in range(self.nb_gradient_steps):\n",
    "            # gradient_step() function in RL5 notebook\n",
    "            if len(self.replay_buffer) > self.batch_size:\n",
    "                states, actions, rewards, new_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "                Q_new_state_max = self.target_model(new_states).max(1)[0].detach()\n",
    "                update = torch.addcmul(rewards, self.gamma, 1 - dones, Q_new_state_max)\n",
    "                Q_s_a = self.model(states).gather(1, actions.to(torch.long).unsqueeze(1))\n",
    "                loss = self.criterion(Q_s_a, update.unsqueeze(1))\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "        \n",
    "        # update target network if needed\n",
    "        if self.time_step_id % self.update_target_freq == 0:\n",
    "            self.target_model.load_state_dict(self.model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2c4ae9",
   "metadata": {},
   "source": [
    "Now let's build a function that run a simulation where a given agent interact with a givent environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a34d76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation(environment, agent, nb_episodes=200, verbose=True):\n",
    "    episodes_rewards_sum = []\n",
    "    agent.on_simulation_start()\n",
    "    for episode_id in range(nb_episodes):\n",
    "        state = environment.reset()\n",
    "        agent.on_episode_start(state)\n",
    "\n",
    "        episode_rewards = []\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.action(state)\n",
    "            state, reward, done, _ = environment.step(action)\n",
    "\n",
    "            # Ending time step process ...\n",
    "            agent.on_action_stop(action, state, reward, done)\n",
    "\n",
    "            # Store reward\n",
    "            episode_rewards.append(reward)\n",
    "        agent.on_episode_stop()\n",
    "        rewards_sum = sum(episode_rewards)\n",
    "        episodes_rewards_sum.append(rewards_sum)\n",
    "        environment.close()\n",
    "        if len(episodes_rewards_sum) > 20:\n",
    "            last_20_average = mean(episodes_rewards_sum[-20:])\n",
    "        else:\n",
    "            last_20_average = mean(episodes_rewards_sum)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Episode \", '{:3d}'.format(episode_id),\n",
    "                  \", episode return \", '{:4.1f}'.format(rewards_sum),\n",
    "                  \", last 20 avg \", '{:4.1f}'.format(last_20_average),\n",
    "                  sep='')\n",
    "    return episodes_rewards_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e333f066",
   "metadata": {},
   "source": [
    "Our agent convergence speed and average performances, will depend on many randomness.\n",
    "To make sure it work and to have a better estimation of it's perfomances, we should run many seeds, many simulations, and observe the average sum of rewards and it's standard deviation over these simulations.\n",
    "\n",
    "This algorithm is also given to you because it's not reinforcement learning and not interesting for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7857968a",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = gym.make(\"CartPole-v1\")\n",
    "nb_seeds = 4\n",
    "\n",
    "\n",
    "seeds_result = []\n",
    "for seed_id in range(nb_seeds):\n",
    "    # Here, don't forget to reset the agent at each seeds!\n",
    "    agent = DQNAgent(environment.observation_space, environment.action_space)\n",
    "    seeds_result.append(simulation(environment, agent))\n",
    "\n",
    "seeds_result = np.array(seeds_result)\n",
    "means = np.mean(seeds_result, axis=0)\n",
    "stds = np.std(seeds_result, axis=0)\n",
    "\n",
    "plt.cla()\n",
    "plt.plot(means, color=\"g\")\n",
    "plt.fill_between([x for x in range(len(means))], means + stds, means - stds, color=\"g\", alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b47bdf",
   "metadata": {},
   "source": [
    "## Actor-critic\n",
    "\n",
    "<div id=\"actor_critic\"></div>\n",
    "\n",
    "As we saw earlier in this class, Deep Q-Network (DQN) was made for discrete actions spaces. \n",
    "It use a Q-network, that is made to give the expected value of each action in a given state.\n",
    "In continuous action space, this algorithm cannot be used because we cannot allocate one neural-network output for each action. \n",
    "To correct this problem, the main idea is to make the Q-Network return a single outpur, that will be the value of the state action pair, that is given in input. But do this prevent us to choose the action, and that's why we add another neural network called the actor, taking a state as the input, and returning the value of each continuous action to take (1 action between -1 and 1 for pendulum, so 1 neuron at the output, 2 for lunar lander).\n",
    "\n",
    "To train this neural network, we just have to use the Q-Network, to estimate how much the chosen action is good to take in the given state.\n",
    "\n",
    "This is the main idea around actor critic architecture. To understand in detail how it work, YOU are going to implement two of the most known off-policy actor critic architectures, Deep-Deterministic Policy Gradient (DDPG) and Soft Actor-Critic (SAC). \n",
    "\n",
    "As in other notebook, the code of these two algorithms are given. But because the number of questions/task to do is low in this notebook compared to others, I hardly recomend you to code them by yourself first. Try to run them, and if it's not working well, try some improvement or to understand what is not working, before to give up and look the solution. You will have the entire session to implement 2 algorithms, so take your time to make sure you understood them well.\n",
    "\n",
    "### Deep-Deterministic Policy Gradient (DDPG)\n",
    "\n",
    "<div id=\"ddpg\"></div>\n",
    "\n",
    "DDPG is considered as an off-policy algorithm, because the policy used to train the critic is different than the one used to generate samples.\n",
    "\n",
    "For the general idea, when we select an interaction (s, a, r, s', a') to train our critic network, the way a' will be chosen will define the reward propagation (generally called credit assignment) speed. If a' is the optimal action from s', like in DQN, there is a high probability to get an action that lead in interestig and highly valuated states s''. In that case, the high values and the rewards will be propagated to (s, a) value, more frequently than s'' with low value.\n",
    "\n",
    "In SARSA case, the a' actions will be chose following GLIE actor (cf RL1 - RL fundamentals notebook), that will depend on an exploration strategy, so a' can be random sometime. When a' is random, we have a high probability to evaluate Q'(s', a') where action a' is a bad action in state s'. In this case, Q(s, a) will also be learned as being a bad action state pair, even if s' can maybe lead to an interesting reward. We call SARSA an on-policy algorithm. For a purely actor-critic on-policy algorithm, you can also check [A3C](https://arxiv.org/pdf/1602.01783.pdf) but you don't need to understand this algorithm in this notebook.\n",
    "\n",
    "If you didn't got it, you can also check this course from Olivier SIGAUD on youtube : https://youtu.be/hlhzvQnXdAA\n",
    "\n",
    "Now, I want you to implement DDPG algorithm, that use an actor network, a critic network, and two target networks for both actor and critic to respectively compute a' and Q'.\n",
    "\n",
    "Note that the default DDPG algorithm don't use epsilon-greedy, but add a noise on the action to make it explore. The original paper use a OU-noise, but for simplicity, we will generate a gaussian noise N(mean, std) to noise our action.\n",
    "\n",
    "To make sure you know how to implement it, let's answer to some questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb3d016",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "If we have an actor network A, a critic actor network A' and a state s, how will I compute actions a to build samples when my agent is interacting with it's environment ? How will I compute action a' to compute Q'(s', a') to train my critic network ?\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533d28b6",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "When the agent is generating training samples while interacting with it's environment, he need to explore it to find new interesting states. In this case, the choosen action will be \n",
    "\n",
    "    a = A(s) + noise\n",
    "    \n",
    "When the agent is generations next_action a' to train critic by computing Q'(s', a'), we don't want him to explore so the generated a' have a higher probability to correspond to a highly valuable Q'(s', a'), so this value have a higher change to be propagated to Q(s, a), speeding up the training.\n",
    "<details/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee5034d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "If the critic is used to evaluate the action a taken by the actor network in a given state s. What will be the loss of the critic network ?\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbab0bd1",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "In this situation, the critic is learning a Q-value, like the DQN model is doing. For this reason, the loss of our critic network should be similar:\n",
    "    \n",
    "    # Pseudo code\n",
    "    target = reward + gamma * (1 - done) * target_critic(s', a')\n",
    "    critic_loss = MSE(critic(s, a), target)\n",
    "<details/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93494b2f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "If the critic is able to evaluate an action a, taken by the actor network in a state s, what should be the actor loss ? (do not search for something to complicated, the answer is simple! :)\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d34950c",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "Note that if we are learning a sample s, a, r, s', d from the buffer, only s can be used in the actor training. In fact, the action chosen by the actor at time step t, will not obviously be the same than the action taken by actor at time step t - n, when the sample was generated in the environment (because the actor is learning at every time step). For this reason, a, r, s', d and no more re-usable for the actor.\n",
    "\n",
    "    a = actor(s)\n",
    "    loss_actor = -critic(s, a)\n",
    "\n",
    "Note that the actor nedd the critic to be trained befor he can learn. In some way, the actor ability is trained to make shure the agent is going in states that satisfy the critic. Generally, in the learn() function, we train the critic before the actor, but is just speed-up the learning by 1 time step so it's not mandatory.\n",
    "<details/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d7e751",
   "metadata": {},
   "source": [
    "#### Performances\n",
    "\n",
    "You shouldn't be worry if your DDPG training is long. In my experiments, the learning took 100 episodes to get a sum of reawards around -700 in average, and 200 episodes to converge between -200 and -400 with the following attributes:\n",
    "\n",
    " - Env: Pendulum-v0\n",
    " - Noise: mean=0, std=0.1\n",
    " - actor_learning_rate=0.000025\n",
    " - critic_learning_rate=0.00025, \n",
    " - gamma=0.99,  # Discount factor\n",
    " - buffer_max_size=1000000, \n",
    " - layer1_size=200, # For both critic and actor\n",
    " - layer2_size=150, # For both critic and actor\n",
    " - batch_size=64,  # For both critic and actor\n",
    " - tau=0.01,\n",
    " \n",
    "The last parameter tau is to update target networks. At each time steps, I update the weights of target networks to be a weighted average between their weights, and the targeted network weights :\n",
    "\n",
    "    target_model_weight = target_model_weight * (1 - tau) + model_weight * tau\n",
    "    \n",
    "This is another way to update target critic and target actor, but the one you learned in RL5 notebook (copy weights every n time steps) should also work.\n",
    "\n",
    "Here I give you an architecture for both actor and critic neural networks. The Sequential function we used before should work, but the learning in continuous action space is really long, and use LayerNorm layers increase so much learning speed, so you will gain time on your notebook. Initialise weights as I do here increase a bit more the learning speed, but according to my experience the impact is not really increadible.\n",
    "\n",
    "[WARN] I also recommand to use torch.tanh activation function on the last layer of the actor since it increase the learning.\n",
    "\n",
    "I can't explain why all of these increase the learning speed, I just observed it guided by some tips found on internet ... Everithing you should understand and remember here, is why the default architecture and behaviour of DDPG make the learning converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce34aab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "def init_weights(layer, bound=None):\n",
    "    if bound is None:\n",
    "        bound = 1. / np.sqrt(layer.weight.data.size()[0])\n",
    "    torch.nn.init.uniform_(layer.weight.data, -bound, bound)\n",
    "    torch.nn.init.uniform_(layer.bias.data, -bound, bound)\n",
    "\n",
    "\n",
    "class DefaultNN(nn.Module):\n",
    "    def __init__(self, learning_rate, input_dims, layer_1_dims, layer_2_dims, output_dims, device,\n",
    "                 last_activation=None):\n",
    "        super().__init__()\n",
    "        self.last_activation = last_activation\n",
    "        self.layer_1 = nn.Linear(input_dims, layer_1_dims)\n",
    "        init_weights(self.layer_1)\n",
    "        self.layer_norm_1 = nn.LayerNorm(layer_1_dims)\n",
    "\n",
    "        self.layer_2 = nn.Linear(layer_1_dims, layer_2_dims)\n",
    "        init_weights(self.layer_2)\n",
    "        self.layer_norm_2 = nn.LayerNorm(layer_2_dims)\n",
    "\n",
    "        self.layer_3 = nn.Linear(layer_2_dims, output_dims)\n",
    "        init_weights(self.layer_3, bound=0.003)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.device = device\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        running_output = self.layer_1(inputs)\n",
    "        running_output = self.layer_norm_1(running_output)\n",
    "        running_output = torch.nn.functional.relu(running_output)\n",
    "        running_output = self.layer_2(running_output)\n",
    "        running_output = self.layer_norm_2(running_output)\n",
    "        running_output = torch.nn.functional.relu(running_output)\n",
    "        running_output = self.layer_3(running_output)\n",
    "\n",
    "        if self.last_activation is not None:\n",
    "            running_output = self.last_activation(running_output)\n",
    "        return running_output\n",
    "\n",
    "    def converge_to(self, other_model, tau=0.01):\n",
    "        \"\"\"\n",
    "        Make the value of parameters of this model converge to one from the given model.\n",
    "        The parameter tau indicate how close our weights should be from the one of the other model.\n",
    "        self.converge_to(other_model, tau=1) is equivalent to self = copy.deepcopy(other_model).\n",
    "\n",
    "        other_model should have the same shape, dimensions, than self.\n",
    "        \"\"\"\n",
    "        for self_param, other_param in zip(self.parameters(), other_model.parameters()):\n",
    "            self_param.data.copy_(\n",
    "                self_param.data * (1.0 - tau) + other_param.data * tau\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24577b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now, implement DDPG. You can take inspiration from DQN, because some behaviours (like replay buffer management) \n",
    "# are still the same here.\n",
    "class DDPGAgent(Agent):\n",
    "    def __init__(self, state_space, action_space, device, actor_lr=0.000025, critic_lr=0.00025, tau=0.01, gamma=0.99,\n",
    "                 max_size=1000000, layer1_size=200, layer2_size=150, batch_size=64, noise_std=0.1, name=\"DDPG\"):\n",
    "        assert isinstance(action_space, gym.spaces.Box)  ### NEW: The action space is now continuous \n",
    "        super().__init__(state_space, action_space, device=device, name=name)\n",
    "        # TODO\n",
    "\n",
    "    def action(self, observation):\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def on_action_stop(self, action, new_state, reward, done):\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def learn(self):\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "# Test our agent on Pendulum-v0\n",
    "environment = gym.make(\"Pendulum-v0\")\n",
    "nb_seeds = 4\n",
    "seeds_result = []\n",
    "for seed_id in range(nb_seeds):\n",
    "    agent = DDPGAgent(environment.observation_space, environment.action_space, device=DEVICE)\n",
    "    seeds_result.append(simulation(environment, agent))\n",
    "\n",
    "seeds_result = np.array(seeds_result)\n",
    "means = np.mean(seeds_result, axis=0)\n",
    "stds = np.std(seeds_result, axis=0)\n",
    "\n",
    "plt.cla()\n",
    "plt.plot(means, color=\"g\")\n",
    "plt.fill_between([x for x in range(len(means))], means + stds, means - stds, color=\"g\", alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e89db5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL6_DDPG.py\n",
    "### NEW indicate the differences between DQN and DDPG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2421c7",
   "metadata": {},
   "source": [
    "### Soft Actor-critic (SAC)\n",
    "\n",
    "<div id=\"sac\"></div>\n",
    "\n",
    "Soft Actor-Critic is really close to DDPG, except that it learn using a regularised entropy on actions policy.\n",
    "\n",
    "#### Shanon entropy\n",
    "\n",
    "Let X be a random variable with a law of density p(X) satisfying the normalization and positivity requirements, we define its entropy by\n",
    "$$-\\int_{X} p(x) log (p(x))$$\n",
    "\n",
    "It allows to quantify the disorder of a random variable. The entropy is maximal when X follows an uniform distribution, and minimal when p(X) is equal to zero everywhere except in one value, which is a Dirac distribution.\n",
    "\n",
    "#### Why entropy in SAC?\n",
    "\n",
    "In SAC algorithm, we not only consider the critic evaluation inside the actor loss, but also it's decision entropy.\n",
    "The loss of the agent should be high when the entropy is low, because it will invite him to explore more.\n",
    "The mode the entropy of the agent is important, the more it will explore.\n",
    "\n",
    "More, SAC use entropy to estimate next_states value. The more the entropy of the target actor is important while choosing a', the more s' will be condidered as highly valuable.\n",
    "To illustrate the intiution behing this, let's imagine an environment with a discrete state space, in a sigle line: \n",
    "\n",
    "\n",
    "|    |    |    |\n",
    "|:--:|:--:|:--:|\n",
    "| +1 | A  | +1 |\n",
    "\n",
    "In the exemple above, the agent (A) will recieve a reward of +1 at the next round, whatever the next action he will chose. Because the actor loss make him maximise critig grade and entropy, he will improve entropy when the critig grage cannot be improved anymore. In this case, the critic value will everytime be the same, whatever the action taken by the agent. For this reason, the action entropy will be maximize, so the agent will have the same probability to go in any directions.\n",
    "\n",
    "This fact bring a side effect. Because states are consider good when the entropy of the action taken from them is high, the agent will maintain itself in states where he will be sake whatever the next taken action. This behavior is also present in DDPG but is strengten here but the entropy maximisation, making our agent gain in stability (you will observe the gap in performances standard deviation latter).\n",
    "\n",
    "\n",
    "|    |    |    |\n",
    "|:--:|:--:|:--:|\n",
    "| -1 | A  | +1 |\n",
    "\n",
    "In this new case, the entropy of the action will be low because the actor loss will be lower if the critic evaluation is high, so he should choose deterministic action that leads to states with the higher value.\n",
    "\n",
    "In other words, we can say that SAC will perform a better exploration/exploitation trade-off, by takin both the advantage of a high exploration, and the advantage of a high exploitation. He will maximize exploration by mawimising entropy, but will not suffer from the incovenient of high exploration that is fall in bad state and never enstrenght the optimal trajectory the the best rewards, because he will fly away from bad rewarding states.\n",
    "\n",
    "\n",
    "#### How to use entropy in SAC?\n",
    "\n",
    "To compute an entropy, we need the actor to give use a random distribution. The most common way to archieve that, is to make the actor with 2 * nb_action outputs neurons, giving actions means and standard deviations.\n",
    "\n",
    "Because the standart deviation, it is common to make the actor return the log(std), and then get the real std with $e^{log_std}$. But we can also put the std output inside a relu().\n",
    "\n",
    "To get the action, we can sample actions from the means and standard deviations we got. Doing this will prevent us latter to retro-propagate the gradient when we will train our actor, so we should do a reparametrization trick when we want to compute actions for actor training (more explanation in the code to fill bellow).\n",
    "Sample with reparametrisation is equivalent to compute $a = \\mu + \\mathcal{N}(0,\\,1)\\ * sigma$.\n",
    "\n",
    "In general, for a given a', we maximise entropy my minimising $log (p(x))$. Looking at the entropy function, we can understand that $log(p(x))$ have a higher impact than p(x) only, because p(x) is between 0 and 1, and $|log(p(x))|$ is generally extremly high because p(x) is generally close to 0.\n",
    "\n",
    "Because our environment action space is bounded, but our normal distribution is not, we can put the actions we got inside tanh function to get some between -1 and 1, and then scale it to our environment action space. Because we bould our actions, we should do a process over our log prob as follow:\n",
    "\n",
    "        log_probs = actions_distribution.log_prob(actions)\n",
    "        log_probs -= torch.log(1 - action.pow(2) + self.min_std)\n",
    "        log_probs = log_probs.sum(dim=-1)\n",
    "        \n",
    "We are not going deeper about these mathematicals details, but if you are interested, the explanation if this is in [SAC paper](https://arxiv.org/pdf/1812.05905.pdf), appendic C (bottom of page 16).\n",
    "\n",
    "Now you should have any informations to complete the code bellow for SAC implementation! To gain some time, wome code parts are already filled.\n",
    "\n",
    "NB: Algorithm hyperparameters are given in $__init__$ function, the new alpha hyper parameter is the ratio between Q value and entropy inside critic update:\n",
    "$$V(s') = TargetCritic(s', a') - alpha * LogProb$$ where $$a', LogProg = SampleAction(s')$$\n",
    "\n",
    "Note that SAC is very stable and converge well, so you don't neet to have low learning rate and you also can use a Sequential() model (like we did with DQN) without LayerNorm layers inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35ef7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACAgent(Agent):\n",
    "    def __init__(self, state_space, action_space, device, actor_lr=0.001, critic_lr=0.001, gamma=0.98, \n",
    "                 max_size=10000, tau=0.005, layer1_size=128, layer2_size=128, batch_size=128, alpha=0.9):\n",
    "        super().__init__(state_space, action_space, device, \"SAC\")\n",
    "        # TODO\n",
    "        pass\n",
    "    \n",
    "    def converge_to(self, model, other_model, tau=None):\n",
    "        \"\"\"\n",
    "        Make the first model weights converge to the second one with a ration of tau.\n",
    "        \"\"\"\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "        for self_param, other_param in zip(model.parameters(), other_model.parameters()):\n",
    "            self_param.data.copy_(\n",
    "                self_param.data * (1.0 - tau) + other_param.data * tau\n",
    "            )\n",
    "\n",
    "    def sample_action(self, state, reparameterize=False, actor_network=None):\n",
    "        if actor_network is None:\n",
    "            actor_network = self.actor\n",
    "        # TODO\n",
    "        # 1. Sample an action from the given state using the given actor network. It can be the target or the \n",
    "        #    default actor depending on when this function is called\n",
    "        # 2. raparameterize is used to performed a reparametrisation trick if we want to keep the gradient \n",
    "        #    to retro-propagate it later. Use reparametrize=True when you call this function to train the actor.\n",
    "        #    if it is True, sample action using distribution.rsampl(), use sample() otherwise.\n",
    "        # 3. Compute the log_probability as explained in SAC description\n",
    "        # 4. Return both the actions taken and the log probabilities\n",
    "        pass\n",
    "\n",
    "    def action(self, state):\n",
    "        actions, _ = self.sample_action(state, reparameterize=False)\n",
    "        return actions.cpu().detach().numpy()\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.replay_buffer) > self.batch_size:\n",
    "            states, actions, rewards, next_states, done = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "            # Training critic\n",
    "            # TODO\n",
    "\n",
    "            # Train actor\n",
    "            # TODO\n",
    "\n",
    "    def on_action_stop(self, action, new_state, reward, done):\n",
    "        self.replay_buffer.append(self.last_state, action, reward, new_state, done)\n",
    "        self.learn()\n",
    "        super().on_action_stop(action, new_state, reward, done)\n",
    "\n",
    "# Test our agent on LunarLanderContinuous-v2  (Don't work on Pendulum because of an unexpected bug)\n",
    "environment = gym.make(\"LunarLanderContinuous-v2\")\n",
    "nb_seeds = 4\n",
    "sac_seeds_result = []\n",
    "ddpg_seeds_result = []\n",
    "for seed_id in range(nb_seeds):\n",
    "    \n",
    "    print()\n",
    "    print(\"###################\")\n",
    "    print()\n",
    "    print(\"      SEED \" + str(seed_id))\n",
    "    print()\n",
    "    print(\"###################\")\n",
    "    \n",
    "    print()\n",
    "    print(\" > Training SAC\")\n",
    "    print()\n",
    "    agent = SACAgent(environment.observation_space, environment.action_space, device=DEVICE)\n",
    "    sac_seeds_result.append(simulation(environment, agent, nb_episodes=80))\n",
    "    \n",
    "    print()\n",
    "    print(\" > Training DDPG\")\n",
    "    print()\n",
    "    agent = DDPGAgent(environment.observation_space, environment.action_space, device=DEVICE)\n",
    "    ddpg_seeds_result.append(simulation(environment, agent, nb_episodes=80))\n",
    "\n",
    "    sac_means = np.mean(np.array(sac_seeds_result), axis=0)\n",
    "    sac_stds = np.std(np.array(sac_seeds_result), axis=0)\n",
    "\n",
    "    ddpg_means = np.mean(np.array(ddpg_seeds_result), axis=0)\n",
    "    ddpg_stds = np.std(np.array(ddpg_seeds_result), axis=0)\n",
    "\n",
    "    plt.cla()\n",
    "    plt.plot(sac_means, color=\"g\", label=\"sac\")\n",
    "    plt.fill_between([x for x in range(len(sac_means))], sac_means + sac_stds, sac_means - sac_stds, color=\"g\", alpha=0.2)\n",
    "    plt.plot(ddpg_means, color=\"r\", label=\"ddpg\")\n",
    "    plt.fill_between([x for x in range(len(ddpg_means))], ddpg_means + ddpg_stds, ddpg_means - ddpg_stds, color=\"b\", alpha=0.2)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68894e45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load solutions/RL6_SAC.py\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0935498c",
   "metadata": {},
   "source": [
    "Now you can see that entropy regularisation improve so much DDPG performances.\n",
    "\n",
    "You don't need to remeber every performance improvement tricks we used in this notebook, but you should understand how DDPG and SAC works, and what are the differences between them, and between DQN and them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
